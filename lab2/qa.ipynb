{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd33f41-1886-4b15-97f9-ee88f09081d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from gliner import GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf1180b-1f82-41e1-a46b-b3ee78a14701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tags_to_spans(samples, tag_to_id):\n",
    "    \"\"\"\n",
    "    Converts NER tags in the dataset samples to spans (start, end, entity type).\n",
    "\n",
    "    Args:\n",
    "        samples (dict): A dictionary containing the tokens and NER tags.\n",
    "        tag_to_id (dict): A dictionary mapping NER tags to IDs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized text and corresponding NER spans.\n",
    "    \"\"\"\n",
    "    ner_tags = samples[\"ner_tags\"]\n",
    "    id_to_tag = {v: k for k, v in tag_to_id.items()}\n",
    "    spans = []\n",
    "    start_pos = None\n",
    "    entity_name = None\n",
    "\n",
    "    for i, tag in enumerate(ner_tags):\n",
    "        if tag_to_id[tag] == 0:  # 'O' tag\n",
    "            if entity_name is not None:\n",
    "                spans.append((start_pos, i - 1, entity_name))\n",
    "                entity_name = None\n",
    "                start_pos = None\n",
    "        else:\n",
    "            tag_name = tag\n",
    "            if tag_name.startswith('B-'):\n",
    "                if entity_name is not None:\n",
    "                    spans.append((start_pos, i - 1, entity_name))\n",
    "                entity_name = tag_name[2:]\n",
    "                start_pos = i\n",
    "            elif tag_name.startswith('I-'):\n",
    "                continue\n",
    "\n",
    "    # Handle the last entity if the sentence ends with an entity\n",
    "    if entity_name is not None:\n",
    "        spans.append((start_pos, len(samples[\"tokens\"]) - 1, entity_name))\n",
    "\n",
    "    return {\"tokenized_text\": samples[\"tokens\"], \"ner\": spans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f940b359-2a6a-40ed-940d-ab68735c9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'adsabs/WIESP2022-NER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d913589-e2a9-4f22-9b6b-81902df99b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e4881a-3ce5-449d-bf5b-6ae2bcd0116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = load_dataset(DATASET_NAME, split='train')\n",
    "testset = load_dataset(DATASET_NAME, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0232726f-923e-403b-a2eb-b593e13d932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for example in trainset[\"ner_tags\"] for tag in example)\n",
    "sorted_tags = sorted(unique_tags - {\"O\"})  # Exclude 'O' from sorted tags\n",
    "list_tags = list(unique_tags)\n",
    "\n",
    "clear = [tag[2:] for tag in list_tags]\n",
    "labels_list = list(dict.fromkeys(clear))\n",
    "\n",
    "tag_to_id = {\"O\": 0, **{tag: idx + 1 for idx, tag in enumerate(sorted_tags)}}\n",
    "id_to_tag = {idx: tag for tag, idx in tag_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c73efd4-b598-491c-a7c9-677d7654aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [ner_tags_to_spans(i, tag_to_id) for i in trainset]\n",
    "test_dataset = [ner_tags_to_spans(i, tag_to_id) for i in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96617df3-cd0d-499b-841c-f1c71c0d212e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1800e16e55c4dadb57d0700786e0fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GLiNER.from_pretrained(\"urchade/gliner_small\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "047cec35-a08b-4eaa-99e8-a1dbd1794c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollator(model.config, data_processor=model.data_processor, prepare_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e689b84-7f8c-4d9a-9d3e-f26a5a7d9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "tokenizers = model.data_processor.transformer_tokenizer\n",
    "tokenizers.model_max_length = 800\n",
    "model.data_processor.config.max_len = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71e02912-7275-4034-b229-769b87dbed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\AppData\\Local\\Temp\\ipykernel_5104\\2913947474.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"E:/tmp/models\",\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    others_lr=1e-5,\n",
    "    others_weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",  # cosine\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    focal_loss_alpha=0.75,\n",
    "    focal_loss_gamma=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=10,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='epoch',\n",
    "    dataloader_num_workers=0,\n",
    "    use_cpu=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizers,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "165f3bf5-d91e-4530-8e36-dc28e2b2f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\AppData\\Local\\Temp\\ipykernel_5104\\897406184.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8cbe2c-101a-4a5b-a439-3d6cc1b47833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in E:\\tmp\\models\\checkpoint-2634\n"
     ]
    }
   ],
   "source": [
    "model = GLiNER.from_pretrained(\"E:/tmp/models/checkpoint-2634\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da18d784-9735-43f0-ae3c-2929735c0146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gliner\\data_processing\\processor.py:269: UserWarning: Sentence of length 822 has been truncated to 800\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "C:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gliner\\data_processing\\processor.py:269: UserWarning: Sentence of length 884 has been truncated to 800\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "C:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gliner\\data_processing\\processor.py:269: UserWarning: Sentence of length 840 has been truncated to 800\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = model.evaluate(\n",
    "    test_dataset, flat_ner=True, entity_types=labels_list, batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed21ae2e-15c8-43d6-a81e-7c0803701f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P: 74.09%\\tR: 74.21%\\tF1: 74.15%\\n', 0.7415358671682448)\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee90688-3fbe-462f-9c1a-bd42cf5d5c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ef39e-808a-4c8e-8dd3-a4aa3a9044ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
